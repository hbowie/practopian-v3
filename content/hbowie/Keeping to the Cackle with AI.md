Title:  Keeping to the Cackle with AI

Tags:   critical thinking, technology, written word

Timestamp: 20230331203558

Status: 9 - Published

Type:   blog

Featured: false

Greatest Hits: false

Category: 07 - Topical

Importance: 85

Date:   31 Mar 2023

Index:  AI; artificial intelligence; Innes, Michael; 

Minutes to Read: 4

Image Name: geese

Image Alt: Group of orange beaked white geese during a golden sunset.

Image Credit: iStock | Jokerbee12

Image Credit Link: https://www.istockphoto.com/photo/white-geese-gm1203003089-345592010

Buttondown Link: https://buttondown.email/practopian/archive/keeping-to-the-cackle-with-ai/

Medium Link: https://medium.com/@hbowie/ai-is-nothing-new-9a14823dadff

Short ID: ainn

Teaser:

Producing speech that sounds meaningful but has no genuine thought behind it is nothing new -- humans have been doing this for years, without any help from newfangled chatbots. 


Body:

No matter where I look these days, it seems [everyone is worried](https://techthings.cmail20.com/t/d-e-zkrdha-idtdldmht-r/) about the dangers of the latest artificial intelligence tools. Here's a typical concern I've heard repeatedly: 

> Oh my gosh, look at that -- this thing can generate very reasonably sounding essays, just by studying which words tend to follow others, and then plugging in one likely word after another! How will teachers ever be able to tell original works from AI-generated text?" 

Well, if you're worried about this, I've got an update for you. 

Producing speech that sounds meaningful but has no genuine thought behind it is nothing new -- humans have been doing this for years, without any help from newfangled chatbots.

To show you what I mean, here's a quote from a mystery novel, *The Seven Suspects*, written by Michael Innes in 1936. Innes himself, it should be noted, was a professor, and the character named Gott, who speaks in the following excerpt, is also an academic. 

> "And your remarks on the text," Mr. Gott declared, "are merely a muddle." 
> 
> "Yes, Gott," said Mike meekly. 
> 
> "You see, Mike, you haven't any *brain* really." 
> 
> "No, of course not," said Mike. 
> 
> "You must just keep to the cackle and write very nicely. You write very nicely." 
> 
> "Yes," said Mike, dubiously. 
> 
> "Keep off thinking things out, and you'll do well. In fact, you'll go far."

Gott's advice, of course, could as well be directed towards any of the latest chatbots in 2023, as it was to Gott's fictional student in 1936. *You haven't any brain really. But just keep to the cackle and write nicely. You'll do well. In fact, you'll go far.*

I also reference Gott's advice in my online book *[The Big Ideas in Software Development](https://www.softdevbigideas.com/this-thinking-business.html)* because, during my forty-plus years of working in corporate Information Technology, I observed so many managers, project leaders and analysts seemingly following the same advice: *keep to the cackle, avoid thinking things out, and you'll do well -- in fact, you'll go far!* 

You see, the latest AI tools are not really doing anything new. They're just doing what many humans have been doing for decades, if not centuries: trying to blend in and receive approval from those around them by repeating the same sort of blather they've heard from others, and that they think others want to hear. 

If you want a modern example of this phenomenon, completely unaided by AI, and produced solely by the human mind, then we need look no farther than the words produced regularly by Donald Trump, and frequently echoed and amplified by other Republican leaders, and the [words regularly spoken on Fox News](https://www.npr.org/2023/03/08/1161694400/fox-news-lawsuit-civil-war-ingraham-hannity). (Or some of the [blather produced on the left these days](https://www.nytimes.com/interactive/2023/02/06/magazine/walter-mosley-interview.html) in the name of being politically correct). 

So is there danger in this sort of speech?

Yes! By keeping to the cackle you will generally fit in, you will sometimes get things right, but you will also experience spectacular failures. Just like the chatbots, you will feel extraordinary confidence in your answers, even if you have no idea what you are actually talking about!

So how can we protect ourselves, as individuals and as a society, from being damaged by this sort of stuff, whether generated by artificial intelligence or the human kind?

1. **Value authenticity.** Spend more time listening to people who are actually speaking from their own thoughts and experiences, and not just parroting others. 

2. **Know your sources.** Learn which ones to trust but, also, learn the strong and weak points of each.  

3. **Learn to tolerate uncertainty.** People who hate being, or appearing, uncertain often tend to seize on the first idea that happens their way and then hang onto it with a death grip.

4. **Spend time thinking things out.** For me, and for many others, writing is part of this process. 

5. **Develop expertise in one or more topics.** Not only will you learn something about those subjects, you will likely develop a greater respect for genuine expertise, and a greater ability to sense when such expertise is being authentically voiced. 

6. **Test your understanding.** Figure out a way to see if the thing being asserted is actually true.

7. **Focus on things that actually matter to you.** This is a companion to the preceding point because, if you have no way to test the veracity or authenticity of a statement, then there is a good chance that the topic is something that doesn't really affect you much one way or the other. 

8. **Spend less time looking at shiny, flashing things.** A big part of our problem is that our modern digital media have become so good at producing empty intellectual calories, and we as readers have become so addicted to their constant consumption. Read a book. Take a walk in the woods. 

The bottom line is that so long as we continue to value cackle, we will be flooded with more of it: whether produced by real people, or AI agents -- or even geese, as pictured above.
